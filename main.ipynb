{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-28T08:58:48.308715Z",
     "start_time": "2023-11-28T08:58:48.306359Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch.nn as nn\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# Directories of the dataset \n",
    "train_original_directory = os.path.join(\"Data\",\"Original_data\",\"train_essays.csv\")\n",
    "test_original_directory = os.path.join(\"Data\",\"Original_data\",\"test_essays.csv\")\n",
    "train_original_prompts_directory = os.path.join(\"Data\",\"Original_data\",\"train_prompts.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T06:56:04.302961Z",
     "start_time": "2023-11-28T06:56:04.299140Z"
    }
   },
   "id": "8fedbfa28d2fc8d0"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "train_original_df = pd.read_csv(train_original_directory)\n",
    "test_original_df = pd.read_csv(test_original_directory)\n",
    "train_original_prompts_df = pd.read_csv(train_original_prompts_directory)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T06:56:04.509438Z",
     "start_time": "2023-11-28T06:56:04.464538Z"
    }
   },
   "id": "bdd3d95a8ed5dc10"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exploratory Data Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba0b400657e5dc3a"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "         id  prompt_id                                               text  \\\n0  0059830c          0  Cars. Cars have been around since they became ...   \n1  005db917          0  Transportation is a large necessity in most co...   \n2  008f63e3          0  \"America's love affair with it's vehicles seem...   \n3  00940276          0  How often do you ride in a car? Do you drive a...   \n4  00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n\n   generated  \n0          0  \n1          0  \n2          0  \n3          0  \n4          0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt_id</th>\n      <th>text</th>\n      <th>generated</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0059830c</td>\n      <td>0</td>\n      <td>Cars. Cars have been around since they became ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>005db917</td>\n      <td>0</td>\n      <td>Transportation is a large necessity in most co...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>008f63e3</td>\n      <td>0</td>\n      <td>\"America's love affair with it's vehicles seem...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00940276</td>\n      <td>0</td>\n      <td>How often do you ride in a car? Do you drive a...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00c39458</td>\n      <td>0</td>\n      <td>Cars are a wonderful thing. They are perhaps o...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generated -> 0 means written by humans \n",
    "train_original_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T06:56:04.691842Z",
     "start_time": "2023-11-28T06:56:04.688772Z"
    }
   },
   "id": "40147f8929ad1d0e"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# Data Cleaning should be independent from the dataLoader (Usefull if we plan to use machine learning model as well\n",
    "# Embedding should be injected into the dataloader class (Makes embedding indepent from the dataloading part \n",
    "# At the moment we don't care about the prompt text but might be usefull in future processing "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T06:56:04.848187Z",
     "start_time": "2023-11-28T06:56:04.834303Z"
    }
   },
   "id": "9fd429f46234c237"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "class CreateDataset:\n",
    "    \"\"\"\n",
    "    This class is just to clean the dataset and the output of this class should be a cleaned dataset\n",
    "    \"\"\"\n",
    "    def __init__(self,values:list = None):\n",
    "        self.__paths : list[str] = []\n",
    "        \n",
    "        if values:\n",
    "            self.__paths = [*values]\n",
    "        \n",
    "    @property\n",
    "    def paths(self):\n",
    "        return self.paths\n",
    "    \n",
    "    @paths.setter\n",
    "    def paths(self,value):\n",
    "        self.paths.append(value)\n",
    "        \n",
    "    def clean(self):\n",
    "        final_df = None\n",
    "        for path in self.paths:\n",
    "            temp_df = pd.read_csv(path)\n",
    "            if(path.split('/') == \"Original\"):\n",
    "                temp_df = self.cleanOriginal(temp_df)\n",
    "            \n",
    "            if final_df is None:\n",
    "                final_df = temp_df\n",
    "            else:\n",
    "                final_df = pd.concat([final_df,temp_df])\n",
    "            \n",
    "    def cleanOriginal(self,temp_df):\n",
    "        # TODO: Drop the promptId and Id\n",
    "        return temp_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T06:56:05.022329Z",
     "start_time": "2023-11-28T06:56:05.018353Z"
    }
   },
   "id": "94ab24a2c1fc3f62"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "temp = CreateDataset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T06:56:05.400104Z",
     "start_time": "2023-11-28T06:56:05.394450Z"
    }
   },
   "id": "e6d499bec1b8c4e8"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "train_final_df = train_original_df[[\"text\",\"generated\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T06:56:05.702149Z",
     "start_time": "2023-11-28T06:56:05.695354Z"
    }
   },
   "id": "838110d584229b6b"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   text  generated\n0     Cars. Cars have been around since they became ...          0\n1     Transportation is a large necessity in most co...          0\n2     \"America's love affair with it's vehicles seem...          0\n3     How often do you ride in a car? Do you drive a...          0\n4     Cars are a wonderful thing. They are perhaps o...          0\n...                                                 ...        ...\n1373  There has been a fuss about the Elector Colleg...          0\n1374  Limiting car usage has many advantages. Such a...          0\n1375  There's a new trend that has been developing f...          0\n1376  As we all know cars are a big part of our soci...          0\n1377  Cars have been around since the 1800's and hav...          0\n\n[1378 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>generated</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Cars. Cars have been around since they became ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Transportation is a large necessity in most co...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"America's love affair with it's vehicles seem...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>How often do you ride in a car? Do you drive a...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Cars are a wonderful thing. They are perhaps o...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1373</th>\n      <td>There has been a fuss about the Elector Colleg...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1374</th>\n      <td>Limiting car usage has many advantages. Such a...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1375</th>\n      <td>There's a new trend that has been developing f...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1376</th>\n      <td>As we all know cars are a big part of our soci...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1377</th>\n      <td>Cars have been around since the 1800's and hav...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1378 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T06:56:05.910604Z",
     "start_time": "2023-11-28T06:56:05.905391Z"
    }
   },
   "id": "9e8521b8b50a0324"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/kaggle/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/deberta-v3-xsmall\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T07:11:54.499908Z",
     "start_time": "2023-11-28T07:11:53.916456Z"
    }
   },
   "id": "cdc6769119fdb705"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "128001"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.get_vocab())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T07:15:49.512458Z",
     "start_time": "2023-11-28T07:15:49.511100Z"
    }
   },
   "id": "c7ec72e3c7665a5b"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "[1,\n 11673,\n 260,\n 11673,\n 286,\n 331,\n 441,\n 515,\n 306,\n 1181,\n 2167,\n 267,\n 262,\n 11537,\n 268,\n 261,\n 335,\n 4166,\n 3692,\n 994,\n 263,\n 1119,\n 262,\n 362,\n 4848,\n 1193,\n 260,\n 11673,\n 286,\n 1313,\n 266,\n 852,\n 985,\n 267,\n 316,\n 469,\n 406,\n 1131,\n 515,\n 393,\n 260,\n 420,\n 394,\n 261,\n 355,\n 281,\n 1392,\n 264,\n 900,\n 337,\n 9781,\n 640,\n 4119,\n 338,\n 282,\n 266,\n 397,\n 576,\n 260,\n 502,\n 351,\n 261,\n 9781,\n 262,\n 380,\n 265,\n 2020,\n 520,\n 282,\n 266,\n 397,\n 576,\n 264,\n 333,\n 260,\n 344,\n 334,\n 912,\n 265,\n 291,\n 261,\n 1030,\n 261,\n 307,\n 2514,\n 2324,\n 97689,\n 261,\n 1982,\n 23905,\n 589,\n 4031,\n 11673,\n 261,\n 309,\n 293,\n 4859,\n 44077,\n 1603,\n 261,\n 361,\n 19835,\n 281,\n 262,\n 107550,\n 265,\n 13550,\n 261,\n 399,\n 1686,\n 938,\n 1549,\n 292,\n 814,\n 9458,\n 289,\n 2460,\n 2670,\n 264,\n 365,\n 308,\n 2002,\n 260,\n 11632,\n 504,\n 361,\n 291,\n 269,\n 266,\n 1288,\n 48368,\n 264,\n 763,\n 1827,\n 264,\n 1684,\n 10326,\n 1698,\n 5685,\n 292,\n 97646,\n 260,\n 28223,\n 2020,\n 281,\n 1744,\n 270,\n 621,\n 864,\n 265,\n 10326,\n 1698,\n 5685,\n 267,\n 1611,\n 260,\n 260,\n 260,\n 715,\n 322,\n 264,\n 960,\n 864,\n 267,\n 347,\n 640,\n 25676,\n 893,\n 267,\n 262,\n 780,\n 1017,\n 260,\n 11673,\n 281,\n 262,\n 872,\n 919,\n 270,\n 262,\n 10326,\n 1698,\n 5685,\n 401,\n 265,\n 266,\n 509,\n 265,\n 355,\n 1785,\n 349,\n 441,\n 305,\n 262,\n 326,\n 646,\n 399,\n 306,\n 389,\n 264,\n 424,\n 260,\n 6219,\n 261,\n 307,\n 37486,\n 24264,\n 1785,\n 775,\n 264,\n 38499,\n 261,\n 309,\n 293,\n 2519,\n 32982,\n 649,\n 652,\n 261,\n 361,\n 3045,\n 261,\n 385,\n 538,\n 265,\n 941,\n 24335,\n 6435,\n 261,\n 15893,\n 266,\n 7245,\n 1785,\n 4797,\n 264,\n 913,\n 262,\n 925,\n 265,\n 262,\n 1307,\n 707,\n 260,\n 325,\n 327,\n 652,\n 261,\n 361,\n 277,\n 1420,\n 261,\n 31889,\n 275,\n 402,\n 45972,\n 2989,\n 6154,\n 332,\n 3233,\n 264,\n 1021,\n 308,\n 2020,\n 288,\n 425,\n 289,\n 282,\n 18855,\n 266,\n 1460,\n 53273,\n 1399,\n 1947,\n 260,\n 279,\n 454,\n 556,\n 338,\n 282,\n 2312,\n 264,\n 5075,\n 45972,\n 6154,\n 262,\n 776,\n 406,\n 260,\n 11673,\n 281,\n 262,\n 919,\n 270,\n 36744,\n 1128,\n 2350,\n 334,\n 3045,\n 260,\n 329,\n 1057,\n 361,\n 966,\n 2020,\n 295,\n 282,\n 401,\n 261,\n 265,\n 305,\n 262,\n 6435,\n 272,\n 306,\n 295,\n 1138,\n 264,\n 299,\n 1128,\n 707,\n 260,\n 11927,\n 261,\n 267,\n 262,\n 1030,\n 261,\n 307,\n 15542,\n 2103,\n 406,\n 269,\n 10852,\n 352,\n 266,\n 610,\n 1139,\n 267,\n 57656,\n 261,\n 309,\n 293,\n 3812,\n 20377,\n 8612,\n 652,\n 261,\n 361,\n 1309,\n 272,\n 280,\n 268,\n 487,\n 264,\n 2443,\n 264,\n 340,\n 1226,\n 261,\n 3543,\n 265,\n 58573,\n 268,\n 31075,\n 261,\n 2772,\n 407,\n 261,\n 61395,\n 261,\n 289,\n 681,\n 262,\n 2444,\n 264,\n 374,\n 482,\n 266,\n 640,\n 2103,\n 406,\n 261,\n 2042,\n 3575,\n 265,\n 291,\n 1909,\n 707,\n 50627,\n 22014,\n 265,\n 1915,\n 25166,\n 260,\n 325,\n 284,\n 262,\n 883,\n 1858,\n 395,\n 2020,\n 286,\n 331,\n 7648,\n 275,\n 364,\n 8369,\n 263,\n 26860,\n 6117,\n 270,\n 262,\n 1286,\n 4031,\n 11673,\n 267,\n 262,\n 1909,\n 707,\n 265,\n 574,\n 705,\n 260,\n 1792,\n 334,\n 262,\n 781,\n 265,\n 591,\n 640,\n 2103,\n 538,\n 401,\n 261,\n 278,\n 1279,\n 349,\n 264,\n 4178,\n 262,\n 6435,\n 272,\n 2020,\n 552,\n 321,\n 265,\n 308,\n 9702,\n 292,\n 355,\n 1785,\n 305,\n 262,\n 326,\n 260,\n 279,\n 1030,\n 327,\n 2920,\n 361,\n 5868,\n 263,\n 1948,\n 4517,\n 286,\n 25747,\n 407,\n 1075,\n 262,\n 707,\n 16916,\n 261,\n 41848,\n 23191,\n 286,\n 331,\n 3473,\n 293,\n 3658,\n 261,\n 2937,\n 23191,\n 6049,\n 4821,\n 5382,\n 286,\n 7603,\n 1174,\n 1915,\n 263,\n 353,\n 3097,\n 263,\n 17954,\n 2017,\n 7226,\n 286,\n 26767,\n 322,\n 260,\n 3014,\n 363,\n 2020,\n 303,\n 331,\n 397,\n 270,\n 262,\n 658,\n 265,\n 4877,\n 401,\n 261,\n 278,\n 303,\n 19522,\n 349,\n 264,\n 2211,\n 479,\n 272,\n 286,\n 858,\n 5334,\n 270,\n 266,\n 455,\n 326,\n 261,\n 1915,\n 25166,\n 286,\n 1699,\n 444,\n 261,\n 263,\n 3097,\n 263,\n 2017,\n 7226,\n 286,\n 12895,\n 322,\n 261,\n 305,\n 775,\n 264,\n 262,\n 713,\n 265,\n 591,\n 625,\n 2020,\n 441,\n 260,\n 344,\n 4533,\n 261,\n 262,\n 380,\n 265,\n 625,\n 2020,\n 263,\n 591,\n 640,\n 2103,\n 538,\n 261,\n 286,\n 330,\n 266,\n 610,\n 1239,\n 277,\n 262,\n 1192,\n 265,\n 2350,\n 401,\n 261,\n 278,\n 269,\n 2947,\n 444,\n 262,\n 925,\n 6435,\n 272,\n 262,\n 2020,\n 286,\n 55899,\n 28612,\n 261,\n 278,\n 303,\n 19522,\n 1226,\n 334,\n 4877,\n 264,\n 2211,\n 23191,\n 261,\n 263,\n 1174,\n 444,\n 1915,\n 25166,\n 260,\n 79853,\n 262,\n 380,\n 265,\n 2020,\n 338,\n 282,\n 266,\n 397,\n 576,\n 270,\n 1121,\n 260,\n 471,\n 301,\n 403,\n 2642,\n 262,\n 380,\n 265,\n 2020,\n 293,\n 1461,\n 4289,\n 266,\n 2772,\n 261,\n 289,\n 1461,\n 2220,\n 3398,\n 272,\n 928,\n 280,\n 297,\n 272,\n 659,\n 292,\n 274,\n 263,\n 702,\n 280,\n 297,\n 389,\n 262,\n 380,\n 265,\n 266,\n 640,\n 264,\n 350,\n 274,\n 343,\n 260,\n 502,\n 351,\n 261,\n 9781,\n 262,\n 380,\n 265,\n 2020,\n 520,\n 282,\n 266,\n 397,\n 576,\n 264,\n 333,\n 260,\n 2]"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(train_final_df.iloc[0][\"text\"])[\"input_ids\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T07:11:54.564424Z",
     "start_time": "2023-11-28T07:11:54.556102Z"
    }
   },
   "id": "b994ac7333622ed6"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "547"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(train_final_df.iloc[1][\"text\"])[\"input_ids\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T07:11:55.584624Z",
     "start_time": "2023-11-28T07:11:55.503436Z"
    }
   },
   "id": "8b99464159cdd758"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "3289"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_final_df.iloc[0][\"text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T07:11:56.024440Z",
     "start_time": "2023-11-28T07:11:56.021080Z"
    }
   },
   "id": "2a6040cbf89e820f"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "2738"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_final_df.iloc[1][\"text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T06:56:08.533626Z",
     "start_time": "2023-11-28T06:56:08.526798Z"
    }
   },
   "id": "2cb9ad8dbcb31edb"
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.pad_token)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T08:47:20.056932Z",
     "start_time": "2023-11-28T08:47:20.054405Z"
    }
   },
   "id": "23bfdf67c0c39d2"
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [],
   "source": [
    "# Create a DataLoader class \n",
    "class DetectionDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,df,Tokenizer = None,train = True,max_length = 100):\n",
    "        self.tokenizer = Tokenizer\n",
    "        self.train = train\n",
    "        self.df = df\n",
    "        self.max_length = max_length\n",
    "        self.padded_token = 0 if self.tokenizer is None else self.tokenizer.convert_tokens_to_ids(self.tokenizer.pad_token)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"\n",
    "        :return: text containing the indexes in numpy array / list\n",
    "        \"\"\"\n",
    "        if self.train:\n",
    "            if self.tokenizer is None: \n",
    "                return {\"text\" : self.df.iloc[item][\"text\"],\"score\" : self.df.iloc[item][\"generated\"]}\n",
    "            else:\n",
    "                vectorized_text = self.vectorize(self.df.iloc[item][\"text\"])\n",
    "                return {\"text\" : vectorized_text,\"score\" : self.df.iloc[item][\"generated\"]}\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    def pad(self,vector,length):\n",
    "        result = np.ones(length) * self.padded_token\n",
    "        result[:len(vector)] = vector\n",
    "        return result\n",
    "    \n",
    "    def collate(self,batch):\n",
    "        max_length = max([len(item['text']) for item in batch])\n",
    "        texts = [self.pad(item['text'],max_length) for item in batch]\n",
    "        scores = [item['score'] for item in batch]    \n",
    "        return {'text': torch.LongTensor(texts), \n",
    "                'score': torch.tensor(scores, dtype=torch.float32)}\n",
    "        \n",
    "    \n",
    "    def vectorize(self,text):\n",
    "        return self.tokenizer(text)[\"input_ids\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T16:07:35.959391Z",
     "start_time": "2023-11-28T16:07:35.958224Z"
    }
   },
   "id": "699912aecea29d4"
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [],
   "source": [
    "detectionDataset = DetectionDataset(train_final_df,tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T16:07:36.600030Z",
     "start_time": "2023-11-28T16:07:36.594015Z"
    }
   },
   "id": "251efd8d0435def9"
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(detectionDataset, batch_size=32, shuffle=True, collate_fn=detectionDataset.collate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T16:07:36.908607Z",
     "start_time": "2023-11-28T16:07:36.903515Z"
    }
   },
   "id": "6c03ad8ebbb485ce"
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [
    {
     "data": {
      "text/plain": "128001"
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.get_vocab())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T16:07:37.321108Z",
     "start_time": "2023-11-28T16:07:37.312630Z"
    }
   },
   "id": "87a6db15e9fc3bfa"
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(128001,1000)\n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x) # B x T X C here B is batch size  T is time step size and \n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T16:36:53.761506Z",
     "start_time": "2023-11-28T16:36:53.732632Z"
    }
   },
   "id": "a36c41bffbb4a890"
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "# class SinusoidalPositionalEmbedding(nn.Embedding):\n",
    "#     \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n",
    "#     def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None) -> None:\n",
    "#         super().__init__(num_positions, embedding_dim)\n",
    "#         self.weight = self._init_weight(self.weight)\n",
    "# \n",
    "#     @staticmethod\n",
    "#     def _init_weight(out: nn.Parameter) -> nn.Parameter:\n",
    "#         \"\"\"\n",
    "#         Interleaved sine and cosine position embeddings\n",
    "#         \"\"\"\n",
    "#         out.requires_grad = False\n",
    "#         out.detach_()\n",
    "# \n",
    "#         N, D = out.shape\n",
    "# \n",
    "#         ## TODO: Create a N x D//2 array of position encodings (argument to the sine/cosine)\n",
    "#         inds = np.arange(0, D // 2)\n",
    "#         k = np.arange(N)\n",
    "#         denom = 1 / np.power(10_000, 2*inds / D)\n",
    "#         position_enc = np.outer(k, denom)  # Efficiently make N x D//2 array for all positions/dimensions\n",
    "#         #####\n",
    "# \n",
    "#         out[:, 0::2] = torch.FloatTensor(np.sin(position_enc))  # Even indices get sin\n",
    "#         out[:, 1::2] = torch.FloatTensor(np.cos(position_enc))  # Odd indices get cos\n",
    "#         return out\n",
    "# \n",
    "#     @torch.no_grad()\n",
    "#     def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0) -> torch.Tensor:\n",
    "#         \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n",
    "#         bsz, seq_len = input_ids_shape[:2]\n",
    "#         positions = torch.arange(\n",
    "#             past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n",
    "#         )\n",
    "#         return super().forward(positions)\n",
    "    \n",
    "class AntHillTransformer(nn.Module):\n",
    "    def __init__(self, d_model=128, n_head=4, n_layers=3):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Input embedding layers\n",
    "        # TODO: Create input embedding and position encoding layers\n",
    "        self.input_embeddings = nn.Embedding(num_embeddings=128001, embedding_dim=d_model, padding_idx=0)\n",
    "        # self.position_encoding = SinusoidalPositionalEmbedding(num_positions=32, embedding_dim=d_model)\n",
    "        ###\n",
    "        self.input_layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Transformer layers\n",
    "        # TODO: Create encoder layer and transformer encoder stack\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.d_model,\n",
    "            nhead=self.n_head,\n",
    "            dim_feedforward=2*d_model,\n",
    "            dropout=0.1,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=self.n_layers)\n",
    "        ###\n",
    "\n",
    "        # Output layers\n",
    "        self.output_norm = nn.LayerNorm(d_model)  # With pre-layer norm transformer, we should do one more before output\n",
    "        # TODO: Create output linear layer\n",
    "        self.output_layer = nn.Linear(128, 1)\n",
    "        ###\n",
    "\n",
    "    def forward(self, input_ids, padding_mask):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        inputs: torch.LongTensor of shape (B, N, D)\n",
    "        padding_mask: torch.BoolTensor of shape (B, N)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        outputs: torch.FlaotTensor of shape (B, N)\n",
    "        \"\"\"\n",
    "        # TODO: Fill in forward layer\n",
    "        # Inputs\n",
    "        embedding_vectors = self.input_embeddings(input_ids)\n",
    "        # position_vectors = self.position_encoding(input_ids.shape)\n",
    "        inputs = embedding_vectors   # Add position encodings\n",
    "        inputs = self.input_layer_norm(inputs)\n",
    "\n",
    "        # Transformer\n",
    "        outputs = self.transformer_encoder(inputs)\n",
    "\n",
    "        # Output layers\n",
    "        outputs = self.output_norm(outputs)\n",
    "        outputs = self.output_layer(outputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        ###\n",
    "        return outputs\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:01:49.477933Z",
     "start_time": "2023-11-28T18:01:49.473980Z"
    }
   },
   "id": "3594ce8f6534efae"
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/kaggle/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = AntHillTransformer()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:01:50.087212Z",
     "start_time": "2023-11-28T18:01:49.909407Z"
    }
   },
   "id": "6ebd3feb8f33a008"
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [],
   "source": [
    "# model = Transformer()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:01:50.670866Z",
     "start_time": "2023-11-28T18:01:50.664404Z"
    }
   },
   "id": "46025c18aea17ace"
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "outputs": [],
   "source": [
    "temp = next(iter(train_dataloader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:01:51.225797Z",
     "start_time": "2023-11-28T18:01:51.163835Z"
    }
   },
   "id": "d3fba77d86963e76"
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "outputs": [],
   "source": [
    "result = model(temp[\"text\"],0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:01:56.882874Z",
     "start_time": "2023-11-28T18:01:51.821517Z"
    }
   },
   "id": "22fdf2a1e51d132b"
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 991])"
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:01:56.889795Z",
     "start_time": "2023-11-28T18:01:56.882353Z"
    }
   },
   "id": "7d09c7fcbec9923d"
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 1085])"
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[\"text\"].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T16:13:56.776929Z",
     "start_time": "2023-11-28T16:13:56.771832Z"
    }
   },
   "id": "213a651054b6bef0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9a911a371cedd816"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
